{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from Dataset.TrainDatasetConstructor import TrainDatasetConstructor\n",
    "from Dataset.EvalDatasetConstructor import EvalDatasetConstructor\n",
    "from eval.eval_as_a_whole import eval_model\n",
    "from metrics import AEBatch, SEBatch\n",
    "import time\n",
    "from model import MaxDilatedCSRNet\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "'SHANGHAITECH': 'A',\n",
    "'min_RATE':10000000,\n",
    "'min_MAE':10240000,\n",
    "'min_MSE':10240000,\n",
    "'eval_num':182,\n",
    "'train_num':300,\n",
    "'learning_rate': 1e-6,\n",
    "'train_batch_size': 1,\n",
    "'epoch': 1000,\n",
    "'eval_per_step': 300,\n",
    "'mode':'whole'\n",
    "}\n",
    "img_dir = \"/home/zzn/Documents/Datasets/part_\" + config['SHANGHAITECH'] + \"_final/train_data/images\"\n",
    "gt_dir = \"/home/zzn/Documents/Datasets/part_\" + config['SHANGHAITECH'] + \"_final/train_data/gt_map\"\n",
    "\n",
    "img_dir_t = \"/home/zzn/Documents/Datasets/part_\" + config['SHANGHAITECH'] + \"_final/test_data/images\"\n",
    "gt_dir_t = \"/home/zzn/Documents/Datasets/part_\" + config['SHANGHAITECH'] + \"_final/test_data/gt_map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"/home/zzn/PycharmProjects/MaxDilation_pytorch/checkpoints/model_1.pkl\"\n",
    "train_dataset = TrainDatasetConstructor(img_dir, gt_dir, train_num=config['train_num'],\n",
    "                 mode=config['mode'],\n",
    "                 if_random_hsi=True,\n",
    "                 if_flip=True)\n",
    "eval_dataset = EvalDatasetConstructor(img_dir_t, gt_dir_t, validate_num=config['eval_num'],\n",
    "                 mode=config['mode'])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=config['train_batch_size'])\n",
    "eval_loader = torch.utils.data.DataLoader(dataset=eval_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the gpu device\n",
    "assert torch.cuda.is_available()\n",
    "cuda_device = torch.device(\"cuda\")\n",
    "\n",
    "net = MaxDilatedCSRNet().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), config['learning_rate'])\n",
    "# optimizer = torch.optim.SGD(net.parameters(), 1e-7, 0.95, 5e-4)\n",
    "criterion = torch.nn.MSELoss(reduction='sum').cuda()\n",
    "ae_batch = AEBatch().cuda()\n",
    "se_batch = SEBatch().cuda()\n",
    "modules = {'model':net, 'loss':criterion, 'ae':ae_batch, 'se':se_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "# torch.cuda.empty_cache()\n",
    "for epoch_index in range(config['epoch']):\n",
    "    dataset = train_dataset.shuffle()\n",
    "    loss_list = []\n",
    "    time_per_epoch = 0\n",
    "    \n",
    "    for train_img_index, train_img, train_gt in train_loader:\n",
    "        if step % config['eval_per_step'] == 0:\n",
    "            validate_MAE, validate_RMSE, time_cost = eval_model(config, eval_loader, modules, True)\n",
    "            sys.stdout.write('In step {}, epoch {}, MAE = {}, MSE = {}, time cost = {}.\\n'.format(step, epoch_index + 1, validate_MAE, validate_RMSE, time_cost))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            #save model\n",
    "            if config['min_MAE'] > validate_MAE:\n",
    "                config['min_MAE'] = validate_MAE\n",
    "                torch.save(net, model_save_path)\n",
    "#             # return train model\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        # B\n",
    "        x = train_img\n",
    "        y = train_gt\n",
    "        start = time.time()\n",
    "        prediction = net(x)\n",
    "        loss = criterion(prediction, y)\n",
    "        loss_list.append(loss.data.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        torch.cuda.synchronize()\n",
    "        end2 = time.time()\n",
    "        time_per_epoch += end2 - start\n",
    "    loss_epoch_mean = np.mean(loss_list)\n",
    "    sys.stdout.write('In step {}, the loss = {}, time_cost_epoch = {}\\n'.format(step, loss_epoch_mean,  time_per_epoch))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
